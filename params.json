{"name":"Pml-prediction assignment","tagline":"Repo for Prediction Assignment from Johns Hopkins' Coursera: Practical Machine Learning","body":"Welcome!\r\n\r\nThis is a short description of the work I did for Prediction Assignment from Johns Hopkins' Coursera: Practical Machine Learning course. Most of description can be found in the comments of the code, but as this is required, I had to do this too.\r\n\r\nFirst thing I did after reading provided data, was to visually analyze it. What quickly strikes out is quite a big numer of obervations (approx. 20k) and features. This proved to be problematic for createDataPArtition function, so I nedded to reduce dimensionality of feature plane. Firstly, I tried to remove rows with missing values, but that left me with observations only from class \"A\". So then, I decided to remove variables with NAs. Together with removal of variable with seemed to be useless for the assignment (X, user_name, new_window, num_window, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp) I was left with 53 variables.\r\n\r\nFor the purpose of building model and tesitng it I decided to randomly sample provided data into 2 partitions: one with approx. 70% of samples as training set and reamining samples as testing set. I made sure that in both sets every class is represented proportionally to it's share in provided, original data.\r\n\r\nFirst classification algorithm I wanted to check was a simple decision tree. This seemed like a reasonable choice, as decision trees are capable of explaining relations bettwen variables and the general flow of the proces (as opposed to 'black-box' approaches like Neural Networks). Unfortunately, both error on training set and cross-validation on testing partition showed rather poor performance (approx. 49%). \r\n\r\nHaving that in mind I decided to go with Random Forrests. Error on the train set looked very promisin (100%). The out of sample error (1-Accuracy_on_testing_partition) was equal to: 1-0.9945=0.0055. Additionally, I could use varImp function for selection of the most important features, but RF gave me 20 out of 20 on first submission, so I decided the model is as good as it could be.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}